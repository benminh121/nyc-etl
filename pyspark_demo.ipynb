{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:40:52.460966Z",
     "start_time": "2025-04-18T06:40:49.774852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "import logging\n",
    "import time\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(\".env\")\n",
    "\n",
    "utils_path = \"/Users/benminh1201/Downloads/nyc_new/utils\"\n",
    "sys.path.append(utils_path)\n",
    "from helpers import load_cfg\n",
    "from minio_utils import MinIOClient\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"s3a://raw/demo\") \\\n",
    "    .config(\"spark.sql.parquet.enableVectorizedReader\", \"false\") \\\n",
    "    .config(\"spark.sql.parquet.writeLegacyFormat\", \"true\") \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .getOrCreate()\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "# Set the MinIO access key, secret key, endpoint, and other configurations\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"benminh1201\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"benminh1201\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/18 18:40:51 WARN Utils: Your hostname, benminh1201.local resolves to a loopback address: 127.0.0.1; using 192.168.1.13 instead (on interface en0)\n",
      "25/04/18 18:40:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/18 18:40:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.catalog.tableExists(\"local.nyc.taxi\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "yellow_tripdata_schema = StructType([\n",
    "    StructField(\"VendorID\", LongType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", DateType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", DateType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"RatecodeID\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True),\n",
    "    StructField(\"airport_fee\", DoubleType(), True)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:41:05.447112Z",
     "start_time": "2025-04-18T06:41:02.682960Z"
    }
   },
   "cell_type": "code",
   "source": "df = spark.read.parquet(\"s3a://processed/iceberg/nyc/yellow_tripdata/data/\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/18 18:41:03 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:25:17.183831Z",
     "start_time": "2025-04-18T06:25:14.634678Z"
    }
   },
   "cell_type": "code",
   "source": "df.count()",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45103875"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df2 = spark.read.parquet(\"s3a://raw/yellow_tripdata/2023/yellow_tripdata_2023-10.parquet\",\n",
    "                         schema=yellow_tripdata_schema)\n",
    "df2.show(5, False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.printSchema()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:41:09.524070Z",
     "start_time": "2025-04-18T06:41:07.374165Z"
    }
   },
   "cell_type": "code",
   "source": "df.show(10)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------------+------------------+---------------+----------------+---------------+-------------+---------+------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+--------------------+-----------+---------+--------------------+-----------+------------+\n",
      "|vendor_id|ratecodeid|pickup_locationId|dropoff_locationId|pickup_datetime|dropoff_datetime|passenger_count|trip_distance|trip_type|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|taxi_type|             trip_id|pickup_year|pickup_month|\n",
      "+---------+----------+-----------------+------------------+---------------+----------------+---------------+-------------+---------+------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+--------------------+-----------+---------+--------------------+-----------+------------+\n",
      "|        1|         1|               48|               234|     2019-02-01|      2019-02-01|            1.0|          2.1|        1|           1|        9.0|  0.5|    0.5|       2.0|         0.0|      0.0|                  0.3|        12.3|                 0.0|       NULL|   yellow|14269b9c7c0f1cef8...|       2019|           2|\n",
      "|        1|         1|              230|                93|     2019-02-01|      2019-02-01|            1.0|          9.8|        1|           2|       32.0|  0.5|    0.5|       0.0|         0.0|      0.0|                  0.3|        33.3|                 0.0|       NULL|   yellow|14269b9c7c0f1cef8...|       2019|           2|\n",
      "|        1|         1|               95|                95|     2019-02-01|      2019-02-01|            1.0|          0.8|        1|           2|        5.5|  0.5|    0.5|       0.0|         0.0|      0.0|                  0.3|         6.8|                 0.0|       NULL|   yellow|14269b9c7c0f1cef8...|       2019|           2|\n",
      "|        1|         1|              140|               263|     2019-02-01|      2019-02-01|            1.0|          0.8|        1|           2|        5.0|  0.5|    0.5|       0.0|         0.0|      0.0|                  0.3|         6.3|                 0.0|       NULL|   yellow|14269b9c7c0f1cef8...|       2019|           2|\n",
      "|        1|         1|              229|               141|     2019-02-01|      2019-02-01|            1.0|          0.8|        1|           2|        4.5|  0.5|    0.5|       0.0|         0.0|      0.0|                  0.3|         5.8|                 0.0|       NULL|   yellow|14269b9c7c0f1cef8...|       2019|           2|\n",
      "|        1|         1|               75|                41|     2019-02-01|      2019-02-01|            1.0|          0.9|        1|           2|        5.0|  0.5|    0.5|       0.0|         0.0|      0.0|                  0.3|         6.3|                 0.0|       NULL|   yellow|14269b9c7c0f1cef8...|       2019|           2|\n",
      "|        1|         1|              246|               229|     2019-02-01|      2019-02-01|            1.0|          2.8|        1|           2|       14.0|  0.5|    0.5|       0.0|         0.0|      0.0|                  0.3|        15.3|                 0.0|       NULL|   yellow|14269b9c7c0f1cef8...|       2019|           2|\n",
      "|        1|         1|               79|               232|     2019-02-01|      2019-02-01|            1.0|          2.1|        1|           2|       10.5|  0.5|    0.5|       0.0|         0.0|      0.0|                  0.3|        11.8|                 0.0|       NULL|   yellow|14269b9c7c0f1cef8...|       2019|           2|\n",
      "|        2|         1|              170|               234|     2019-01-31|      2019-01-31|            1.0|         0.49|        1|           1|        4.0|  0.5|    0.5|       1.7|         0.0|      0.0|                  0.3|         7.0|                 0.0|       NULL|   yellow|57e7335733eaae516...|       2019|           1|\n",
      "|        2|         1|              107|               161|     2019-01-31|      2019-01-31|            1.0|         1.61|        1|           1|        8.0|  0.5|    0.5|      2.32|         0.0|      0.0|                  0.3|       11.62|                 0.0|       NULL|   yellow|57e7335733eaae516...|       2019|           1|\n",
      "+---------+----------+-----------------+------------------+---------------+----------------+---------------+-------------+---------+------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+--------------------+-----------+---------+--------------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "df = df.select(\n",
    "    # identifiers\n",
    "    col(\"VendorID\").cast(\"long\").alias(\"vendor_id\"),\n",
    "    col(\"RatecodeID\").cast(\"int\").alias(\"ratecodeid\"),\n",
    "    col(\"PULocationID\").cast(\"int\").alias(\"pickup_locationId\"),\n",
    "    col(\"DOLocationID\").cast(\"int\").alias(\"dropoff_locationId\"),\n",
    "    # timestamps\n",
    "    col(\"tpep_pickup_datetime\")\n",
    "    .cast(\"DATE\")\n",
    "    .alias(\"pickup_datetime\"),\n",
    "    col(\"tpep_dropoff_datetime\")\n",
    "    .cast(\"DATE\")\n",
    "    .alias(\"dropoff_datetime\"),\n",
    "    # trip info\n",
    "    col(\"passenger_count\").cast(\"double\").alias(\"passenger_count\"),\n",
    "    col(\"trip_distance\").cast(\"double\").alias(\"trip_distance\"),\n",
    "    lit(1).alias(\"trip_type\"),\n",
    "    # payment info\n",
    "    col(\"payment_type\").cast(\"int\").alias(\"payment_type\"),\n",
    "    col(\"fare_amount\").cast(\"double\").alias(\"fare_amount\"),\n",
    "    col(\"extra\").cast(\"double\").alias(\"extra\"),\n",
    "    col(\"mta_tax\").cast(\"double\").alias(\"mta_tax\"),\n",
    "    col(\"tip_amount\").cast(\"double\").alias(\"tip_amount\"),\n",
    "    col(\"tolls_amount\").cast(\"double\").alias(\"tolls_amount\"),\n",
    "    lit(0.0).alias(\"ehail_fee\"),\n",
    "    col(\"improvement_surcharge\")\n",
    "    .cast(\"double\")\n",
    "    .alias(\"improvement_surcharge\"),\n",
    "    col(\"total_amount\").cast(\"double\").alias(\"total_amount\"),\n",
    "    col(\"congestion_surcharge\")\n",
    "    .cast(\"double\")\n",
    "    .alias(\"congestion_surcharge\"),\n",
    "    col(\"airport_fee\").cast(\"double\").alias(\"airport_fee\"),\n",
    ")\n",
    "df.show(5, False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, year\n",
    "\n",
    "# get year\n",
    "df = df.withColumn('pickup_year', year(col('tpep_pickup_datetime')))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "df_Columns = [\"airport_fee\"]\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_Columns]\n",
    "          ).show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Start with the original DataFrame\n",
    "df = df.withColumn(\"vendor_id\", col(\"VendorID\").cast(\"long\"))\n",
    "df = df.withColumn(\"ratecodeid\", col(\"RatecodeID\").cast(\"double\"))\n",
    "df = df.withColumn(\"pickup_locationId\", col(\"PULocationID\").cast(\"int\"))\n",
    "df = df.withColumn(\"dropoff_locationId\", col(\"DOLocationID\").cast(\"int\"))\n",
    "\n",
    "# timestamps\n",
    "df = df.withColumn(\"pickup_datetime\", col(\"tpep_pickup_datetime\").cast(\"timestamp\"))\n",
    "df = df.withColumn(\"dropoff_datetime\", col(\"tpep_dropoff_datetime\").cast(\"timestamp\"))\n",
    "\n",
    "# trip info\n",
    "df = df.withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"double\"))\n",
    "df = df.withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"double\"))\n",
    "df = df.withColumn(\"trip_type\", lit(1))\n",
    "\n",
    "# payment info\n",
    "df = df.withColumn(\"payment_type\", col(\"payment_type\").cast(\"int\"))\n",
    "df = df.withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"double\"))\n",
    "df = df.withColumn(\"extra\", col(\"extra\").cast(\"double\"))\n",
    "df = df.withColumn(\"mta_tax\", col(\"mta_tax\").cast(\"double\"))\n",
    "df = df.withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"double\"))\n",
    "df = df.withColumn(\"tolls_amount\", col(\"tolls_amount\").cast(\"double\"))\n",
    "df = df.withColumn(\"ehail_fee\", lit(0.0))\n",
    "df = df.withColumn(\"improvement_surcharge\", col(\"improvement_surcharge\").cast(\"double\"))\n",
    "df = df.withColumn(\"total_amount\", col(\"total_amount\").cast(\"double\"))\n",
    "df = df.withColumn(\"congestion_surcharge\", col(\"congestion_surcharge\").cast(\"double\"))\n",
    "df = df.withColumn(\"airport_fee\", col(\"airport_fee\").cast(\"double\"))\n",
    "\n",
    "# Drop the original columns that were renamed\n",
    "columns_to_drop = [\"VendorID\", \"RatecodeID\", \"PULocationID\", \"DOLocationID\",\n",
    "                   \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "df = df.drop(*columns_to_drop)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, concat_ws, sha2, unix_timestamp, lit\n",
    "\n",
    "df = df.select(\n",
    "    # identifiers\n",
    "    col(\"VendorID\").cast(\"long\").alias(\"vendor_id\"),\n",
    "    col(\"RatecodeID\").cast(\"double\").alias(\"ratecodeid\"),\n",
    "    col(\"PULocationID\").cast(\"int\").alias(\"pickup_locationId\"),\n",
    "    col(\"DOLocationID\").cast(\"int\").alias(\"dropoff_locationId\"),\n",
    "    # timestamps\n",
    "    col(\"tpep_pickup_datetime\").cast(\"timestamp\").alias(\"pickup_datetime\"),\n",
    "    col(\"tpep_dropoff_datetime\").cast(\"timestamp\").alias(\"dropoff_datetime\"),\n",
    "    # trip info\n",
    "    col(\"passenger_count\").cast(\"double\").alias(\"passenger_count\"),\n",
    "    col(\"trip_distance\").cast(\"double\").alias(\"trip_distance\"),\n",
    "    lit(1).alias(\"trip_type\"),\n",
    "    # payment info\n",
    "    col(\"payment_type\").cast(\"int\").alias(\"payment_type\"),\n",
    "    col(\"fare_amount\").cast(\"double\").alias(\"fare_amount\"),\n",
    "    col(\"extra\").cast(\"double\").alias(\"extra\"),\n",
    "    col(\"mta_tax\").cast(\"double\").alias(\"mta_tax\"),\n",
    "    col(\"tip_amount\").cast(\"double\").alias(\"tip_amount\"),\n",
    "    col(\"tolls_amount\").cast(\"double\").alias(\"tolls_amount\"),\n",
    "    lit(0.0).alias(\"ehail_fee\"),\n",
    "    col(\"improvement_surcharge\").cast(\"double\").alias(\"improvement_surcharge\"),\n",
    "    col(\"total_amount\").cast(\"double\").alias(\"total_amount\"),\n",
    "    col(\"congestion_surcharge\").cast(\"double\").alias(\"congestion_surcharge\"),\n",
    "    col(\"airport_fee\").cast(\"double\").alias(\"airport_fee\"),\n",
    ")\n",
    "df = df.withColumn(\"taxi_type\", lit(\"yellow\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.printSchema()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.show(5, False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"s3a://raw/demo/\") \\\n",
    "    .save()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df2 = spark.read.parquet(\"s3a://processed/yellow_tripdata/*\")\n",
    "df2.show(5, False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df2.writeTo(\"local.nyc.taxi\").createOrReplace()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load packages\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# define boundaries\n",
    "startdate = datetime.strptime('2019-01-01', '%Y-%m-%d')\n",
    "enddate = datetime.strptime('2025-01-01', '%Y-%m-%d')\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# define column names and its transformation rules on the Date column\n",
    "column_rule_df = spark.createDataFrame([\n",
    "    (\"DateID\", \"cast(date_format(date, 'yyyyMMdd') as int)\"),  # 20230101\n",
    "    (\"Year\", \"year(date)\"),  # 2023\n",
    "    (\"Quarter\", \"quarter(date)\"),  # 1\n",
    "    (\"Month\", \"month(date)\"),  # 1\n",
    "    (\"Day\", \"day(date)\"),  # 1\n",
    "    (\"Week\", \"weekofyear(date)\"),  # 1\n",
    "    (\"QuarterNameShort\", \"date_format(date, 'QQQ')\"),  # Q1\n",
    "    (\"QuarterNumberString\", \"date_format(date, 'QQ')\"),  # 01\n",
    "    (\"MonthNameLong\", \"date_format(date, 'MMMM')\"),  # January\n",
    "    (\"MonthNameShort\", \"date_format(date, 'MMM')\"),  # Jan\n",
    "    (\"MonthNumberString\", \"date_format(date, 'MM')\"),  # 01\n",
    "    (\"DayNumberString\", \"date_format(date, 'dd')\"),  # 01\n",
    "    (\"WeekNameLong\", \"concat('week', lpad(weekofyear(date), 2, '0'))\"),  # week 01\n",
    "    (\"WeekNameShort\", \"concat('w', lpad(weekofyear(date), 2, '0'))\"),  # w01\n",
    "    (\"WeekNumberString\", \"lpad(weekofyear(date), 2, '0')\"),  # 01\n",
    "    (\"DayOfWeek\", \"dayofweek(date)\"),  # 1\n",
    "    (\"YearMonthString\", \"date_format(date, 'yyyy/MM')\"),  # 2023/01\n",
    "    (\"DayOfWeekNameLong\", \"date_format(date, 'EEEE')\"),  # Sunday\n",
    "    (\"DayOfWeekNameShort\", \"date_format(date, 'EEE')\"),  # Sun\n",
    "    (\"DayOfMonth\", \"cast(date_format(date, 'd') as int)\"),  # 1\n",
    "    (\"DayOfYear\", \"cast(date_format(date, 'D') as int)\")  # 1\n",
    "], [\"new_column_name\", \"expression\"])\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# explode dates between the defined boundaries into one column\n",
    "start = int(startdate.timestamp())\n",
    "stop = int(enddate.timestamp())\n",
    "df = spark.range(start, stop, 60 * 60 * 24).select(col(\"id\").cast(\"timestamp\").cast(\"date\").alias(\"Date\"))\n",
    "df.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this loops over all rules defined in column_rule_df adding the new columns\n",
    "for row in column_rule_df.collect():\n",
    "    new_column_name = row[\"new_column_name\"]\n",
    "    expression = expr(row[\"expression\"])\n",
    "    df = df.withColumn(new_column_name, expression)\n",
    "display(df)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# display(df.withColumn(\"Playground\", expr(\"date_format(date, 'yyyyMMDD')\")))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for row in column_rule_df.collect():\n",
    "    print(row)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
